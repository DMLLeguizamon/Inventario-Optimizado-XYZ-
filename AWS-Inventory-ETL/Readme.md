ğŸš€ AWS Inventory ETL Pipeline

ğŸ“Š DescripciÃ³n del Proyecto

ğŸ—ï¸ Arquitectura y Flujo de Datos
En este proyecto se implementÃ³ una arquitectura de procesamiento de datos en la nube utilizando servicios de AWS, organizada en etapas de ingesta, almacenamiento, transformaciÃ³n y consumo.

Ingesta (Capa Raw): Se utilizan distintas fuentes de datos (clientes, ERP, servidores). Mediante una funciÃ³n AWS Lambda en Python, se consumen APIs para extraer la informaciÃ³n y almacenarla en Amazon S3 (Raw Data) en su estado original.

CatalogaciÃ³n (Metadatos):
AWS Glue Crawler analiza los archivos en S3 y detecta automÃ¡ticamente su estructura. Los metadatos (columnas, tipos de datos, esquemas) se registran en AWS Glue Data Catalog.

TransformaciÃ³n (ETL):
Mediante AWS Glue Jobs, se aplican reglas de limpieza:

ValidaciÃ³n y manejo de nulos.

Ajuste de tipos de datos.

CorrecciÃ³n de inconsistencias.
Los datos limpios se depositan en Amazon S3 (Clear Data).

Carga y NormalizaciÃ³n (Capa SQL):
Una segunda AWS Lambda detecta los datos en la capa Clear y ejecuta un proceso de carga hacia SQL Database mediante scripts predefinidos, automatizando la carga que anteriormente era manual.
