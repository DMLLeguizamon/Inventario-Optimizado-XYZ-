{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd819cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job  # A√±adido: Manejo formal del Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# CONFIGURACI√ìN Y PAR√ÅMETROS\n",
    "args = getResolvedOptions(sys.argv,\n",
    "                          ['JOB_NAME',\n",
    "                           'DATABASE',\n",
    "                           'OUTPUT_PATH'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args) # Inicializaci√≥n obligatoria para tracking en AWS\n",
    "\n",
    "database = args['DATABASE']\n",
    "output_path = args['OUTPUT_PATH']\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Iniciando proceso ETL basado en Contratos de Datos\")\n",
    "\n",
    "# CONTRATOS DE DATOS (Metadata Driven)\n",
    "DATA_CONTRACTS = {\n",
    "    \"Precios_compras_2017\": {\n",
    "        \"int\": [\"Brand\", \"Classification\", \"VendorNumber\"],\n",
    "        \"float\": [\"Price\", \"PurchasePrice\"],\n",
    "        \"string\": [\"Description\", \"Size\", \"Volume\", \"VendorName\"],\n",
    "        \"date\": []\n",
    "    },\n",
    "    \"Inicio_inventario\": {\n",
    "        \"int\": [\"Store\", \"Brand\", \"onHand\"],\n",
    "        \"float\": [\"Price\"],\n",
    "        \"string\": [\"InventoryId\", \"City\", \"Description\", \"Size\"],\n",
    "        \"date\": [\"startDate\"]\n",
    "    },\n",
    "    \"Final_inventario\": {\n",
    "        \"int\": [\"Store\", \"Brand\", \"onHand\"],\n",
    "        \"float\": [\"Price\"],\n",
    "        \"string\": [\"InventoryId\", \"City\", \"Description\", \"Size\"],\n",
    "        \"date\": [\"endDate\"]\n",
    "    },\n",
    "    \"Facturas_compras\": {\n",
    "        \"int\": [\"VendorNumber\", \"PONumber\", \"Quantity\"],\n",
    "        \"float\": [\"Dollars\", \"Freight\"],\n",
    "        \"string\": [\"VendorName\", \"Approval\"],\n",
    "        \"date\": [\"InvoiceDate\", \"PODate\", \"PayDate\"]\n",
    "    },\n",
    "    \"Compra_final\": {\n",
    "        \"int\": [\"Store\", \"Brand\", \"VendorNumber\", \"PONumber\", \"Quantity\", \"Classification\"],\n",
    "        \"float\": [\"PurchasePrice\", \"Dollars\"],\n",
    "        \"string\": [\"InventoryId\", \"Description\", \"Size\", \"VendorName\"],\n",
    "        \"date\": [\"PODate\", \"ReceivingDate\", \"InvoiceDate\", \"PayDate\"]\n",
    "    },\n",
    "    \"Venta_final\": {\n",
    "        \"int\": [\"Store\", \"Brand\", \"SalesQuantity\", \"Volume\", \"Classification\", \"VendorNo\"],\n",
    "        \"float\": [\"SalesDollars\", \"SalesPrice\", \"ExciseTax\"],\n",
    "        \"string\": [\"InventoryId\", \"Description\", \"Size\", \"VendorName\"],\n",
    "        \"date\": [\"SalesDate\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# FUNCIONES DE LIMPIEZA Y TRANSFORMACI√ìN\n",
    "\n",
    "def cast_columns(df, contract):\n",
    "    \"\"\"Aplica tipado fuerte seg√∫n el contrato\"\"\"\n",
    "    for col_name in contract[\"int\"]:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(IntegerType()))\n",
    "    for col_name in contract[\"float\"]:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    for col_name in contract[\"date\"]:\n",
    "        # Se asume formato est√°ndar, 'coerce' nativo de Spark al castear\n",
    "        df = df.withColumn(col_name, to_date(col(col_name), \"yyyy-MM-dd\"))\n",
    "    return df\n",
    "\n",
    "def clean_strings(df, contract):\n",
    "    \"\"\"Estandariza textos: sin espacios extra y todo en may√∫sculas\"\"\"\n",
    "    for col_name in contract[\"string\"]:\n",
    "        df = df.withColumn(col_name, trim(upper(col(col_name))))\n",
    "    return df\n",
    "\n",
    "def impute_nulls(df, contract):\n",
    "    \"\"\"Estrategia de imputaci√≥n: Mediana para n√∫meros, constantes para el resto\"\"\"\n",
    "    numeric_cols = contract[\"int\"] + contract[\"float\"]\n",
    "    for col_name in numeric_cols:\n",
    "        if col_name in df.columns:\n",
    "            # Calculamos mediana de forma aproximada para optimizar recursos\n",
    "            median_val = df.approxQuantile(col_name, [0.5], 0.01)\n",
    "            if median_val:\n",
    "                df = df.fillna({col_name: median_val[0]})\n",
    "\n",
    "    for col_name in contract[\"string\"]:\n",
    "        df = df.fillna({col_name: \"UNKNOWN\"})\n",
    "    for col_name in contract[\"date\"]:\n",
    "        df = df.fillna({col_name: \"1900-01-01\"})\n",
    "    return df\n",
    "\n",
    "def treat_outliers(df, contract):\n",
    "    \"\"\"Capping de valores at√≠picos usando el Rango Intercuart√≠lico (IQR)\"\"\"\n",
    "    numeric_cols = contract[\"int\"] + contract[\"float\"]\n",
    "    for col_name in numeric_cols:\n",
    "        if col_name in df.columns:\n",
    "            quantiles = df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "            q1, q3 = quantiles[0], quantiles[1]\n",
    "            iqr = q3 - q1\n",
    "            lower, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "\n",
    "            df = df.withColumn(\n",
    "                col_name,\n",
    "                when(col(col_name) < lower, lower)\n",
    "                .when(col(col_name) > upper_bound, upper_bound)\n",
    "                .otherwise(col(col_name))\n",
    "            )\n",
    "    return df\n",
    "\n",
    "# EJECUCI√ìN DEL PROCESO\n",
    "\n",
    "tables_to_process = list(DATA_CONTRACTS.keys())\n",
    "\n",
    "for table_name in tables_to_process:\n",
    "    try:\n",
    "        logger.info(f\"Procesando tabla: {table_name}\")\n",
    "\n",
    "        # Lectura desde el Diccionario de Datos (Data Catalog)\n",
    "        dynamic_frame = glueContext.create_dynamic_frame.from_catalog(\n",
    "            database=database,\n",
    "            table_name=table_name\n",
    "        )\n",
    "        df = dynamic_frame.toDF()\n",
    "\n",
    "        if df.count() == 0:\n",
    "            logger.warning(f\"La tabla {table_name} est√° vac√≠a. Saltando...\")\n",
    "            continue\n",
    "\n",
    "        # Aplicaci√≥n de transformaciones en cadena\n",
    "        contract = DATA_CONTRACTS[table_name]\n",
    "        df = cast_columns(df, contract)\n",
    "        df = clean_strings(df, contract)\n",
    "        df = impute_nulls(df, contract)\n",
    "        df = treat_outliers(df, contract)\n",
    "\n",
    "        # Escritura en Capa CLEAR (Formato Parquet optimizado)\n",
    "        # Se usa 'overwrite' para que el Job sea re-ejecutable sin duplicar data\n",
    "        target_path = f\"{output_path}/{table_name}\"\n",
    "        df.write.mode(\"overwrite\").format(\"parquet\").save(target_path)\n",
    "\n",
    "        logger.info(f\"‚úÖ {table_name} procesada y guardada en {target_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error procesando {table_name}: {str(e)}\")\n",
    "\n",
    "# Finalizaci√≥n formal del Job para liberar recursos\n",
    "job.commit()\n",
    "logger.info(\"üèÅ ETL COMPLETO FINALIZADO CON √âXITO\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
